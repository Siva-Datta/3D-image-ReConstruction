## Testset RGB to PointCloud

Input RGB image from test set - indoor.png <br/>
![input](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/indoor.png)

Output PointCloud <br/>

![output](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/pointcloud.gif)<br/>

## Random Sample RGB to 3D mesh 

Input Image - sample.jpeg <br/>
![sample](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/sample.jpeg)<br/>

Mesh with humans flattened and pulled to zero depth - pull.gif <br/>
![pull](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/pull.gif)<br/>

Mesh with humans flattened but in the average depth - averagae.gif <br/>
![average](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/average.gif)<br/>

Mesh with humans also in 3D and pulled a little bit (directly proportional to people segmentation output) to seperate from background and give a pop effect - pop.gif <br/>
![pop](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/pop.gif)<br/>
