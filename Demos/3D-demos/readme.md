## Testset RGB to PointCloud

Input RGB image from test set - indoor.png
![input](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/indoor.png)

Output PointCloud

![output](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/pointcloud.gif)

## Random Sample RGB to 3D mesh 

Input Image - sample.jpeg
![sample](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/sample.jpeg)

Mesh with humans flattened and pulled to zero depth - pull.gif
![pull](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/pull.gif)

Mesh with humans flattened but in the average depth - averagae.gif
![average](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/average.gif)

Mesh with humans also in 3D and pulled a little bit (directly proportional to people segmentation output) to seperate from background and give a pop effect - pop.gif
![pop](https://github.com/sivadatta-ss20/3D-image-understanding-Construction/blob/master/Demos/3D-demos/pop.gif)
