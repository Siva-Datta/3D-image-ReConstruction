{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DepthEstimate\n",
    "model = DepthEstimate()\n",
    "batch_size     = 16\n",
    "learning_rate  = 0.0001\n",
    "epochs         = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, csv_file='data/nyu2_train.csv', DEBUG=False):\n",
    "        self.shape_rgb = (480,640, 3)\n",
    "        self.shape_depth = (240,320, 1)\n",
    "        self.read_nyu_data(csv_file, DEBUG=DEBUG)\n",
    "    def read_nyu_data(self, csv_file, DEBUG=False):\n",
    "        csv = open(csv_file, 'r').read()\n",
    "        nyu2_train = list((row.split(',') for row in (csv).split('\\n') if len(row) > 0))\n",
    "\n",
    "        # Dataset shuffling happens here\n",
    "        nyu2_train = shuffle(nyu2_train, random_state=0)\n",
    "\n",
    "        # Test on a smaller dataset\n",
    "        if DEBUG: nyu2_train = nyu2_train[:10]\n",
    "        \n",
    "        # A vector of RGB filenames.\n",
    "        self.filenames = [i[0] for i in nyu2_train]\n",
    "\n",
    "        # A vector of depth filenames.\n",
    "        self.labels = [i[1] for i in nyu2_train]\n",
    "\n",
    "        # Length of dataset\n",
    "        self.length = len(self.filenames)\n",
    "\n",
    "    def _parse_function(self, filename, label): \n",
    "        # Read images from disk\n",
    "        #image_decoded = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
    "        depth_resized = tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(label)), [self.shape_depth[0], self.shape_depth[1]])\n",
    "        image_decoded = tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(filename)), [self.shape_rgb[0], self.shape_rgb[1]])\n",
    "        # Format\n",
    "        rgb = tf.image.convert_image_dtype(image_decoded, dtype=tf.float32)\n",
    "        depth = tf.image.convert_image_dtype(depth_resized / 255.0, dtype=tf.float32)\n",
    "        \n",
    "        # Normalize the depth values (in cm)\n",
    "        depth = 1000 / tf.clip_by_value(depth * 1000, 10, 1000)\n",
    "\n",
    "        return rgb, depth\n",
    "\n",
    "    def get_batched_dataset(self, batch_size):\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((self.filenames, self.labels))\n",
    "        self.dataset = self.dataset.shuffle(buffer_size=len(self.filenames), reshuffle_each_iteration=True)\n",
    "        self.dataset = self.dataset.repeat()\n",
    "        self.dataset = self.dataset.map(map_func=self._parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        #self.dataset = self.dataset.map(map_func=self.nyu_resize)\n",
    "        self.dataset = self.dataset.batch(batch_size=batch_size)\n",
    "\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader()\n",
    "train_generator = dl.get_batched_dataset(batch_size)\n",
    "\n",
    "print('Data loader ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def depth_loss_function(y_true, y_pred, theta=0.3, maxDepthVal=1000.0/10.0):\n",
    "    \n",
    "    # Point-wise depth\n",
    "    l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)\n",
    "\n",
    "    # Structural similarity (SSIM) index\n",
    "    l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, maxDepthVal)) * 0.5, 0, 1)\n",
    "\n",
    "    # Weights\n",
    "    w1 = 1.0\n",
    "    w3 = theta\n",
    "\n",
    "    return (w1 * l_ssim)  + (w3 * K.mean(l_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "#from loss import depth_loss_function\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.Adam(lr=learning_rate, amsgrad=True)\n",
    "\n",
    "model.compile(loss=depth_loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from io import BytesIO\n",
    "rgb = np.load('eigen_test_rgb.npy')\n",
    "depth = np.load('eigen_test_depth.npy')\n",
    "crop = np.load('eigen_test_crop.npy')\n",
    "print('Test data loaded.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"~/cp.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import predict, load_images, display_images, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "b=time.time()\n",
    "e = evaluate(model, rgb, depth, crop, batch_size=1)\n",
    "print(\"Pretrained Weights - eval results \")\n",
    "print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format('a1', 'a2', 'a3', 'rel', 'rms', 'log_10'))\n",
    "print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(e[0],e[1],e[2],e[3],e[4],e[5]))\n",
    "print(\"eval time...\")\n",
    "print(time.time()-b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ev_callback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    b=time.time()\n",
    "    e = evaluate(model, rgb1, depth1, crop, batch_size=1)\n",
    "    print(\"eval results \")\n",
    "    print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format('a1', 'a2', 'a3', 'rel', 'rms', 'log_10'))\n",
    "    print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(e[0],e[1],e[2],e[3],e[4],e[5]))\n",
    "    print(\"eval time...\")\n",
    "    print(time.time()-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow\n",
    "\n",
    "checkpoint_path = \"~pathto store checkpoints/cp.ckpt\"\n",
    "\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tensorflow.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "\n",
    "history=model.fit(train_generator, epochs=epochs, steps_per_epoch=dl.length//batch_size, callbacks=[cp_callback,ev_callback()],initial_epoch=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
