{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Model\n",
    "Use one of the model files in the models directory as model.py (default to mobilenetV2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n",
      "Base model loaded\n",
      "\n",
      "Model created.\n"
     ]
    }
   ],
   "source": [
    "from model import DepthEstimate\n",
    "model = DepthEstimate()                   \n",
    "batch_size     = 16                       \n",
    "learning_rate  = 0.0001\n",
    "epochs         = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def depth_loss_function(y_true, y_pred, theta=0.3, maxDepthVal=1000.0/10.0):\n",
    "    \n",
    "    # Point-wise depth\n",
    "    l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)\n",
    "\n",
    "    # Structural similarity (SSIM) index\n",
    "    l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, maxDepthVal)) * 0.5, 0, 1)\n",
    "\n",
    "    # Weights\n",
    "    w1 = 1.0\n",
    "    w3 = theta\n",
    "\n",
    "    return (w1 * l_ssim) + (w2 * K.mean(l_edges)) + (w3 * K.mean(l_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "#from loss import depth_loss_function   \n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.Adam(lr=learning_rate, amsgrad=True)\n",
    "\n",
    "model.compile(loss=depth_loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Loading weights\n",
    " Load weights available in the google drive corresponding to the model loaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f219939d350>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"~/cp.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the DepthMap\n",
    "Output is saved as out.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "# Read Images \n",
    "rgb = mpimg.imread('/path-to-input-image/')  # Load input image\n",
    "rgb = rgb.resize((480,640))   #Resize Input image to 480x640\n",
    "# Output Images \n",
    "plt.imshow(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import time\n",
    "import matplotlib\n",
    "a    = time.time()\n",
    "img1 = rgb\n",
    "img1 = img1.astype('float32')\n",
    "out = predict(model,img1,10.0,1000.0)\n",
    "out = out.reshape(240,320)\n",
    "out = scipy.ndimage.zoom(out, 2, order=1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(out)\n",
    "plt.show()\n",
    "matplotlib.image.imsave(\"~/out.png\",out,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation \n",
    "We used the gluoncv library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/jakeret/unet.git\n",
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics\n",
    "import unet\n",
    "from unet.datasets import oxford_iiit_pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "unet_model = unet.build_model(*oxford_iiit_pet.IMAGE_SIZE,\n",
    "                              channels=oxford_iiit_pet.channels,\n",
    "                              num_classes=oxford_iiit_pet.classes,\n",
    "                              layer_depth=5,\n",
    "                              filters_root=64,\n",
    "                              padding=\"same\"\n",
    "                              )\n",
    "\n",
    "unet.finalize_model(unet_model,\n",
    "                    loss=losses.SparseCategoricalCrossentropy(),\n",
    "                    metrics=[metrics.SparseCategoricalAccuracy()],\n",
    "                    auc=False,\n",
    "                    learning_rate=LEARNING_RATE)\n",
    "train_dataset, validation_dataset = oxford_iiit_pet.load_data()\n",
    "trainer = unet.Trainer(name=\"oxford_iiit_pet\", checkpoint_callback=True)\n",
    "\n",
    "trainer.fit(unet_model,\n",
    "            train_dataset,\n",
    "            validation_dataset,\n",
    "            epochs=25,\n",
    "            batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "im = Image.open(r'C:\\Users\\datta\\Downloads\\sample.jpeg')\n",
    "im = im.resize((128,128))\n",
    "num=np.array(im)/255\n",
    "num=num.reshape(1,128,128,3)\n",
    "import tensorflow as tf\n",
    "numtf = tf.convert_to_tensor(num, np.float32)\n",
    "prediction = unet_model.predict(numtf)\n",
    "gray = np.dot(prediction[0][...,:3], [0.2989, 0.5870, 0.1140])\n",
    "gray_im = Image.fromarray(np_im)\n",
    "gray_im = gray_im.resize((480,640))\n",
    "predict = np.array(gray_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixles inside the segmented area are divided by a certain factor to separate them from background and give a pop effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predict)):\n",
    "    for j in range(len(predict[0])):\n",
    "        if predict[i][j]>avg:\n",
    "                out1[i][j]= out1[i][j]/1.25\n",
    "        else:\n",
    "            out1[i][j]=out[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can Also push all the pixels in the segmented area to \"min_depth\" to get the average-3D mode as discussed in demo (Please refer to 3D demo directory under Demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB + DepthMap to PointClouds\n",
    "We are using the open3d library here.\n",
    "Final pointcloud is saved as pcd.ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "out1=out1.astype('float32')\n",
    "color = o3d.geometry.Image(rgb)                                   \n",
    "depth = o3d.geometry.Image(out1)\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(color, depth,convert_rgb_to_intensity=False)\n",
    "print(rgbd_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('RGB image')\n",
    "plt.imshow(rgbd_image.color)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('depth image')\n",
    "plt.imshow(rgbd_image.depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "        rgbd_image,\n",
    "        o3d.camera.PinholeCameraIntrinsic(\n",
    "            o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault))\n",
    "\n",
    "pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "o3d.io.write_point_cloud(\"~/pcd.ply\", pcd)\n",
    "o3d.visualization.draw_geometries([pcd],window_name='a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
